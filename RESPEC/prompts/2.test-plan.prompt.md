You are a senior test engineer. Your goal is to design a minimal, high-signal pytest suite that validates current observable behavior and is sufficient to guide a clean reimplementation.

Authoritative inputs:
1) respec\out\1.system-doc.md (index of features/interfaces; do not treat implementation details as requirements unless externally observable)
2) The project source code (ground truth)

Scope exclusions (mandatory):
- Ignore any folders that start with "__".
- Ignore ALL existing tests and test infrastructure (e.g., tests/, test_*.py, conftest.py, fixtures, snapshots, CI test configs). Do not read them, do not mirror their structure, and do not reuse their test cases or assertions.

Constraints:
- Prefer black-box tests (CLI/API/filesystem) over unit tests.
- Do NOT invent edge cases; include additional behaviors only if explicitly implemented and observably different.
- Do NOT aim for exhaustive coverage; target ~12â€“25 tests max.
- Assertions must be stable: avoid timestamps, random IDs, log text, and ordering unless enforced.
- Use pytest. Use subprocess for CLI. Use HTTP client for web (requests or httpx). Use temporary directories for isolation.
- Tests SHOULD NOT assert internal implementation details

Data verification requirements (for operations that produce persisted data):
- For each data-producing operation, tests MUST verify:
  1. Operation completion (exit code, HTTP status)
  2. Artifact existence (file created, table has rows)
  3. **Data shape verification** (expected columns populated, no unexpected nulls)
  4. **Transformation verification** (enriched/computed fields differ from raw input when applicable)
- Example: An extraction test should verify not just "turns table has rows" but also:
  - Enriched fields are populated (e.g., `response_time_ms`, `cleaned_text_tokens`)
  - Conditional side-effect tables have rows when triggered (e.g., `code_metrics` when edits present)

Task:
1) Propose a minimal test suite structure (folders/files) that is independent of any existing test structure.
2) List test cases grouped by tiers:
   - Tier 0: harness/fixtures
   - Tier 1: core happy-path behavior
   - Tier 2: critical inter-boundary/contract tests (only where necessary for reimplementation confidence)
For each test case, include:
- Purpose
- Setup
- Stimulus (command/request/action)
- Observable assertions (precise but not brittle)
- Mapping to relevant feature(s)/section(s) in system-doc.md

Side-effect inventory (required for data-producing operations):
Before defining tests for any operation that modifies state, enumerate:
1. **All side effects** (tables modified, files created, state changes)
2. **Classify each**: primary (always happens) vs conditional (depends on input)
3. **Require at least one assertion per primary side effect**
4. **Require at least one fixture+test for each conditional side effect**

Example inventory for `--extract`:
| Side Effect | Type | Fixture Needed | Assertion |
|-------------|------|----------------|----------|
| db.db created | primary | - | file exists |
| turns table rows | primary | messages in source | count > 0 |
| code_metrics rows | conditional | edits in source | count > 0 when edits present |
| response_time_ms populated | primary (assistant) | - | not null for assistant turns |
| cleaned_text differs | conditional | long text with noise | cleaned != original |

Output format:
- Numbered list of proposed tests
- Suggested test directory layout
- Assumptions/prerequisites (e.g., sample data availability, environment variables)

Output:
- Write the final output as Markdown to: respec\out\2.test-plan.md
- Do not include any text outside that file.
